Brynn LeBlanc, Marius Scheuller, Marcus Kamen, Daniel Yakubu
Artificial intelligence project 2









Decision Tree Thinking / Methodology:
In order to decide which themes are most important, we must calculate the entropy of each theme,
and choose the one with the least entropy. The formula for entropy is probability of class * absolute value of the
log base 2 of the probability of the class. Comparing to our examples in class, the C values are the utility ranges,
where  The 5 ranges are ‘great’ is equivalent to [0.8, 1.0], ‘good’ is equivalent to [0.6, 0.8), ‘ok’ is equivalent to
[0.4, 0.6), ‘low’ is equivalent to [0.2, 0.4), and ‘very low’ is equivalent to [0.0, 0.2). You will preprocess the data
so that each Uk is replaced by one of the 5 labels from ‘great’ to ‘very low’.The 'V' values are the frequency of the
themes (from 0-10). In order to do this we iterate through each theme frequency for each utility range. 



Decision Tree Parameterization
1. Utility ranges and number of classes. There is some hard-coding of ranges in label mapping. Fix that
2. Allow user pass in any impurity function (gini, information gain or even custom functions)